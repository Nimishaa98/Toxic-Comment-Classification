# Toxic-Comment-Classification
In this notebook, we will be fine tuning a transformer model for the Multilabel text classification problem. This is one of the most common business problems where a given piece of text/sentence/document needs to be classified into one or more of categories out of the given list.
Here, the model will detect different types of toxicity in comments such as threats, obscenity, insults, and identity-based hate. So we need to classify input sentence into 6 categories i.e.
1. toxic
2. severe_toxic
3. obscene
4. threat
5. insult
6. identity_hate